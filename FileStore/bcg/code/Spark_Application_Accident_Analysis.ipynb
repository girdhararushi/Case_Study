{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee26656c-7399-43fd-8894-1ab39ca0ed5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae082b03-9041-4085-9a1d-0c5b7cba7ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Accident Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d12eab-3c67-4db2-bd3c-5d625cdc5a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading config file\")\n",
    "\n",
    "json_data = spark.read.json(\"dbfs:/FileStore/bcg/config/config.json\",multiLine=True)\n",
    "\n",
    "charges_path = json_data.select(\"Charges_use\").head()[0]\n",
    "damages_path = json_data.select(\"Damages_use\").head()[0]\n",
    "endorse_path = json_data.select(\"Endorse_use\").head()[0]\n",
    "person_path = json_data.select(\"Primary_Person_use\").head()[0]\n",
    "restrict_path = json_data.select(\"Restrict_use\").head()[0]\n",
    "units_path = json_data.select(\"Units_use\").head()[0]\n",
    "output_dir = json_data.select(\"Output_Dir\").head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517e1d5d-c978-4847-a7be-43f5160df778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data files\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data files\")\n",
    "\n",
    "charges_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(charges_path)\n",
    "damages_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(damages_path)\n",
    "endorse_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(endorse_path)\n",
    "person_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(person_path)\n",
    "restrict_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(restrict_path)\n",
    "units_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(units_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c64c60e-bdfa-4326-aee6-6668f9bade00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388be4c5-ac05-4db1-ae92-d45bc77f6272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 1 Output:\n3000\n"
     ]
    }
   ],
   "source": [
    "#Analysis 1: Find the number of crashes (accidents) in which number of males killed are greater than 2\n",
    "\n",
    "def analysis1():\n",
    "    modified_person_df = person_df.filter(trim(col(\"prsn_gndr_id\")) == \"MALE\").groupBy(\"crash_id\").count().filter(col(\"count\")>2)\n",
    "    return modified_person_df.count()\n",
    "\n",
    "output1 = analysis1()\n",
    "print(\"Analysis 1 Output:\")\n",
    "print(output1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad9a5a7-6c80-41d4-983e-f01852f0a533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 2 Output:\n773\n"
     ]
    }
   ],
   "source": [
    "#Analysis 2: How many two wheelers are booked for crashes?\n",
    "\n",
    "def analysis2():\n",
    "    modified_units_df = units_df.filter(col(\"VEH_BODY_STYL_ID\").like(\"%MOTORCYCLE%\")).select(\"crash_id\",\"unit_nbr\").distinct()\n",
    "    return modified_units_df.count()\n",
    "output2 = analysis2()\n",
    "print(\"Analysis 2 Output:\")\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12434046-8736-4225-b9b2-ec50c16395ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 3 Output:\n+-----------+-----+\n|veh_make_id|count|\n+-----------+-----+\n|     NISSAN|    4|\n|  CHEVROLET|    3|\n|       FORD|    2|\n|      HONDA|    2|\n|    PONTIAC|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Analysis 3: Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy\n",
    "\n",
    "def analysis3():\n",
    "    person_df1 = person_df.filter((trim(col(\"prsn_type_id\")) == \"DRIVER\") & (trim(col(\"prsn_airbag_id\")) == \"NOT DEPLOYED\") & (trim(col(\"PRSN_INJRY_SEV_ID\")) == \"KILLED\"))\n",
    "    \n",
    "    units_df2 = units_df.filter(col(\"veh_body_styl_id\").like('%CAR%'))\n",
    "\n",
    "    join_df = person_df1.join(units_df2, (person_df1[\"crash_id\"] == units_df2[\"crash_id\"]) & (person_df1[\"unit_nbr\"] == units_df2[\"unit_nbr\"]), \"inner\").select(units_df2[\"crash_id\"],units_df2[\"unit_nbr\"],units_df2[\"veh_make_id\"]).distinct()\n",
    "    \n",
    "    final_df = join_df.groupBy(\"veh_make_id\").count().orderBy(\"count\",ascending=False).limit(5)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "output3 = analysis3()\n",
    "\n",
    "print(\"Analysis 3 Output:\")\n",
    "output3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db82cdb9-7ea0-44be-8fb8-7cea02c66c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 4 Output:\n36\n"
     ]
    }
   ],
   "source": [
    "#Analysis 4: Determine number of Vehicles with driver having valid licences involved in hit and run?\n",
    "\n",
    "def analysis4():\n",
    "    person_df2 = person_df.filter( (trim(col(\"prsn_type_id\")).isin('DRIVER','DRIVER OF MOTORCYCLE TYPE VEHICLE')) & (trim(col(\"drvr_lic_type_id\")).isin('DRIVER LICENSE','COMMERCIAL DRIVER LIC.')) )\n",
    "    \n",
    "    modified_charges_df = charges_df.filter((col(\"charge\").like('%HIT AND RUN%')) | (col(\"charge\").like('%HIT & RUN%')))\n",
    "    \n",
    "    return person_df2.join(modified_charges_df, (person_df2[\"crash_id\"] == modified_charges_df[\"crash_id\"]) & (person_df2[\"unit_nbr\"] == modified_charges_df[\"unit_nbr\"])).count()\n",
    "\n",
    "output4 = analysis4()\n",
    "\n",
    "print(\"Analysis 4 Output:\")\n",
    "print(output4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32774e27-872c-43f6-a890-b4cd26471600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 5 Output:\nTexas\n"
     ]
    }
   ],
   "source": [
    "#Analysis 5: Which state has highest number of accidents in which females are not involved\n",
    "\n",
    "def analysis5():\n",
    "    person_df3 = person_df.filter(~col(\"prsn_gndr_id\").isin(\"FEMALE\")).select(\"crash_id\",\"drvr_lic_state_id\").distinct().groupBy(\"drvr_lic_state_id\").count().orderBy(col(\"count\"),ascending=False).select(\"drvr_lic_state_id\")\n",
    "\n",
    "    return person_df3.first()[\"drvr_lic_state_id\"]\n",
    "\n",
    "output5 = analysis5()\n",
    "\n",
    "print(\"Analysis 5 Output:\")\n",
    "print(output5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3054969-55f8-4bc7-9dd6-54e6428d274c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 6 Output:\n+-----------+\n|veh_make_id|\n+-----------+\n|     TOYOTA|\n|      DODGE|\n|     NISSAN|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Analysis 6: Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death\n",
    "\n",
    "def analysis6():\n",
    "    person_df4 = person_df.select(\"crash_id\",\"unit_nbr\",expr(\"tot_injry_cnt+death_cnt\").alias(\"inj_cnt\"))\n",
    "    \n",
    "    units_df1 = units_df.select(\"crash_id\",\"unit_nbr\",\"veh_make_id\").distinct()\n",
    "    \n",
    "    join_df = person_df4.join(units_df1,(person_df4[\"crash_id\"] == units_df[\"crash_id\"]) & (person_df4[\"unit_nbr\"] == units_df[\"unit_nbr\"]),\"inner\")\n",
    "    \n",
    "    mod_join_df = join_df.groupBy(\"veh_make_id\").agg(expr(\"sum(inj_cnt)\").alias(\"inj_sum\"))\n",
    "    \n",
    "    windowSp = Window.orderBy(desc(\"inj_sum\"))\n",
    "    \n",
    "    final_df = mod_join_df.withColumn(\"rnk\", rank().over(windowSp)).filter((col(\"rnk\")>=3) & (col(\"rnk\")<=5)).select(\"veh_make_id\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "output6 = analysis6()\n",
    "\n",
    "print(\"Analysis 6 Output:\")\n",
    "output6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780dfbce-3ffc-404a-8b81-0636322cd2ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 7 Output:\n+--------------------+-----------------+\n|    veh_body_styl_id|prsn_ethnicity_id|\n+--------------------+-----------------+\n|           AMBULANCE|            WHITE|\n|                 BUS|            BLACK|\n|      FARM EQUIPMENT|            WHITE|\n|          FIRE TRUCK|            WHITE|\n|          MOTORCYCLE|            WHITE|\n|                  NA|            WHITE|\n|NEV-NEIGHBORHOOD ...|            WHITE|\n|        NOT REPORTED|            WHITE|\n|OTHER  (EXPLAIN I...|            WHITE|\n|PASSENGER CAR, 2-...|            WHITE|\n|PASSENGER CAR, 4-...|            WHITE|\n|              PICKUP|            WHITE|\n|    POLICE CAR/TRUCK|            WHITE|\n|   POLICE MOTORCYCLE|            WHITE|\n|SPORT UTILITY VEH...|            WHITE|\n|               TRUCK|            WHITE|\n|       TRUCK TRACTOR|            WHITE|\n|             UNKNOWN|          UNKNOWN|\n|                 VAN|            WHITE|\n|   YELLOW SCHOOL BUS|            BLACK|\n+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Analysis 7: For all the body styles involved in crashes, mention the top ethnic user group of each unique body style\n",
    "\n",
    "def analysis7():\n",
    "    units_bs_df = units_df.select(\"crash_id\",\"unit_nbr\",\"veh_body_styl_id\").distinct()\n",
    "    \n",
    "    join_df = person_df.join(units_bs_df, (person_df[\"crash_id\"] == units_bs_df[\"crash_id\"]) & (person_df[\"unit_nbr\"] == units_bs_df[\"unit_nbr\"]),\"inner\")\n",
    "    \n",
    "    grouped_df = join_df.groupBy(\"veh_body_styl_id\",\"prsn_ethnicity_id\").count()\n",
    "    \n",
    "    windowSp = Window.partitionBy(\"veh_body_styl_id\").orderBy(desc(col(\"count\")))\n",
    "    \n",
    "    final_df = grouped_df.withColumn(\"rnk\", rank().over(windowSp)).filter(col(\"rnk\") == 1).select(\"veh_body_styl_id\",\"prsn_ethnicity_id\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "output7 = analysis7()\n",
    "\n",
    "print(\"Analysis 7 Output:\")\n",
    "output7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad92260-59bd-4615-95ee-7e7714524b86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 8 Output:\n+--------+-----+\n|drvr_zip|count|\n+--------+-----+\n|   76010|   26|\n|   75067|   25|\n|   75052|   25|\n|   78521|   21|\n|   78745|   19|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Analysis 8: Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)\n",
    "\n",
    "def analysis8():\n",
    "    units_al_df = units_df.select(\"crash_id\",\"unit_nbr\",\"veh_body_styl_id\",\"contrib_factr_1_id\").filter((col(\"veh_body_styl_id\").like('%CAR%')) & ((trim(col(\"contrib_factr_1_id\")) == 'UNDER INFLUENCE - ALCOHOL') | (trim(col(\"contrib_factr_2_id\")) == 'UNDER INFLUENCE - ALCOHOL'))).distinct()\n",
    "    \n",
    "    person_df5=person_df.withColumnRenamed(\"CRASH_ID\",\"PERSON_CRASH_ID\")\n",
    "    \n",
    "    join_df = person_df5.join(units_al_df, (person_df5[\"person_crash_id\"] == units_al_df[\"crash_id\"]) & (person_df5[\"unit_nbr\"] == units_al_df[\"unit_nbr\"]),\"inner\")\n",
    "    \n",
    "    final_df = join_df.groupBy(\"drvr_zip\").agg(countDistinct(col(\"person_crash_id\")).alias(\"count\")).orderBy(col(\"count\"),ascending=False).filter(col(\"drvr_zip\").isNotNull()).limit(5)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "output8 = analysis8()\n",
    "\n",
    "print(\"Analysis 8 Output:\")\n",
    "output8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4aa0194-a820-4a37-a356-019ecc4d29cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 9 Output:\n5531\n"
     ]
    }
   ],
   "source": [
    "#Analysis 9: Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance\n",
    "\n",
    "def analysis9():\n",
    "    units_df5 = units_df.filter(((col(\"veh_dmag_scl_1_id\").isin('DAMAGED 5','DAMAGED 6','DAMAGED 7 HIGHEST')) | (col(\"veh_dmag_scl_2_id\").isin('DAMAGED 5','DAMAGED 6','DAMAGED 7 HIGHEST'))) & (col(\"FIN_RESP_TYPE_ID\").like('%INSURANCE%')) & (col(\"veh_body_styl_id\").like('%CAR%'))).withColumnRenamed(\"CRASH_ID\",\"UNITS_CRASH_ID\")\n",
    "    \n",
    "    mod_damages_df = damages_df.filter(~trim(col(\"damaged_property\")).isin(\"NONE\",\"NONE1\"))\n",
    "    join_df = units_df5.join(mod_damages_df, units_df5[\"units_crash_id\"] == mod_damages_df[\"crash_id\"],\"left_anti\")\n",
    "\n",
    "    damages_none_df = damages_df.filter(trim(col(\"damaged_property\")).isin(\"NONE\",\"NONE1\")).select(\"crash_id\").distinct()\n",
    "    \n",
    "    return join_df.select(\"UNITS_CRASH_ID\").union(damages_none_df).distinct().count()\n",
    "\n",
    "output9 = analysis9()\n",
    "\n",
    "print(\"Analysis 9 Output:\")\n",
    "print(output9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf72f597-6372-4701-9450-1db4246b2b16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis 10 Output:\n+-----------+-----+\n|veh_make_id|count|\n+-----------+-----+\n|     TOYOTA| 1405|\n|       FORD| 1270|\n|  CHEVROLET| 1150|\n|      HONDA| 1108|\n|     NISSAN|  844|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Analysis 10: Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)\n",
    "\n",
    "def analysis10():\n",
    "    top_states = units_df.select(\"crash_id\",\"veh_lic_state_id\").distinct().groupBy(\"veh_lic_state_id\").count().orderBy(\"count\",ascending=False).limit(25).select(\"veh_lic_state_id\").withColumnRenamed(\"veh_lic_state_id\",\"top_veh_lic_state_id\")\n",
    "    \n",
    "    top_colous = units_df.select(\"crash_id\",\"unit_nbr\",\"veh_color_id\").distinct().groupBy(\"veh_color_id\").count().orderBy(col(\"count\"),ascending=False).filter(col(\"veh_color_id\") != 'NA').limit(10).select(\"veh_color_id\")\n",
    "    \n",
    "    trans_units_df = units_df.select(\"crash_id\",\"unit_nbr\",\"veh_body_styl_id\",\"veh_color_id\",\"veh_lic_state_id\",\"veh_make_id\").distinct().withColumnRenamed(\"crash_id\",\"units_crash_id\").withColumnRenamed(\"unit_nbr\",\"unit_own_nbr\").filter(col(\"veh_body_styl_id\").like('%CAR%')).join(top_states, units_df[\"veh_lic_state_id\"] == top_states[\"top_veh_lic_state_id\"])\n",
    "    \n",
    "    trans_units_df2 = trans_units_df.join(top_colous, trans_units_df[\"veh_color_id\"] == top_colous[\"veh_color_id\"])\n",
    "    \n",
    "    lic_drivers = person_df.filter( (trim(col(\"prsn_type_id\")).isin('DRIVER','DRIVER OF MOTORCYCLE TYPE VEHICLE')) & (trim(col(\"drvr_lic_type_id\")).isin('DRIVER LICENSE','COMMERCIAL DRIVER LIC.')) )\n",
    "    \n",
    "    trans_units_df3 = trans_units_df2.join(lic_drivers, (trans_units_df2[\"units_crash_id\"] == lic_drivers[\"crash_id\"]) & (trans_units_df2[\"unit_own_nbr\"] == lic_drivers[\"unit_nbr\"]))\n",
    "    \n",
    "    speed_df = charges_df.filter(col(\"charge\").like('%SPEED%')).select(\"crash_id\",\"unit_nbr\").distinct()\n",
    "    \n",
    "    trans_units_df4 = trans_units_df3.join(speed_df, (trans_units_df3[\"units_crash_id\"] == speed_df[\"crash_id\"]) & (trans_units_df3[\"unit_own_nbr\"] == speed_df[\"unit_nbr\"]))\n",
    "    \n",
    "    final_df = trans_units_df4.select(\"unit_own_nbr\",\"units_crash_id\",\"veh_make_id\").groupBy(\"veh_make_id\").count().orderBy(col(\"count\"),ascending=False).limit(5)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "output10 = analysis10()\n",
    "\n",
    "print(\"Analysis 10 Output:\")\n",
    "output10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e7947a-4e27-451d-bf8d-b9a8ab2823f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data for ANALYSIS 1\nWriting data for ANALYSIS 2\nWriting data for ANALYSIS 3\nWriting data for ANALYSIS 4\nWriting data for ANALYSIS 5\nWriting data for ANALYSIS 6\nWriting data for ANALYSIS 7\nWriting data for ANALYSIS 8\nWriting data for ANALYSIS 9\nWriting data for ANALYSIS 10\nData Writing Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing data for ANALYSIS 1\")\n",
    "\n",
    "spark.createDataFrame([output1], StringType()).withColumnRenamed(\"value\",\"output1\").write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output1.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 2\")\n",
    "spark.createDataFrame([output2], StringType()).withColumnRenamed(\"value\",\"output2\").write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output2.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 3\")\n",
    "output3.write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output3.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 4\")\n",
    "spark.createDataFrame([output4], StringType()).withColumnRenamed(\"value\",\"output4\").write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output4.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 5\")\n",
    "spark.createDataFrame([output5], StringType()).withColumnRenamed(\"value\",\"output5\").write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output5.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 6\")\n",
    "output6.write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output6.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 7\")\n",
    "output7.write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output7.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 8\")\n",
    "output8.write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output8.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 9\")\n",
    "spark.createDataFrame([output9], IntegerType()).withColumnRenamed(\"value\",\"output9\").write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output9.csv\")\n",
    "\n",
    "print(\"Writing data for ANALYSIS 10\")\n",
    "output10.write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_dir+\"/output10.csv\")\n",
    "\n",
    "print(\"Data Writing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b1fa4d-0735-4057-8dbf-89af3387dd69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Application_Accident_Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
